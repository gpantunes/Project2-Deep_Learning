{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09621229-d756-4908-9564-93385bd4c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Environment.snake_game import SnakeGame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import imageio # For video generation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "GAME_WIDTH = 20\n",
    "GAME_HEIGHT = 20\n",
    "FOOD_AMOUNT = 1\n",
    "GRASS_GROWTH = 0.001\n",
    "MAX_GRASS = 0.05\n",
    "BORDER_SIZE = 1 # Add a visual border of 1 pixel\n",
    "MAX_GAME_STEPS = 500 # Set a limit for the game length\n",
    "VIDEO_FILENAME = \"snake_ai_player.gif\"\n",
    "FPS = 5 # Faster for smoother video\n",
    "\n",
    "TEMPERATURE_END = 0.1\n",
    "TEMPERATURE_START = 2.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf6f037-36c5-4308-8725-92fd45d8d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Heuristic Player\n",
    "\"\"\"\n",
    "class SnakeHeuristic:\n",
    "    def __init__(self, game: SnakeGame):\n",
    "        self.game = game\n",
    "        # Map: game_direction_idx -> (row_change, col_change)\n",
    "        self.delta_coords = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (0, 1),   # Right\n",
    "            2: (1, 0),   # Down\n",
    "            3: (0, -1)   # Left\n",
    "        }\n",
    "\n",
    "    def _get_closest_apple_pos(self):\n",
    "        if not self.game.apples:\n",
    "            return None\n",
    "        \n",
    "        head_r, head_c = self.game.snake[0]\n",
    "        closest_apple = None\n",
    "        min_dist = float('inf')\n",
    "\n",
    "        for apple_r, apple_c in self.game.apples:\n",
    "            dist = abs(apple_r - head_r) + abs(apple_c - head_c)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest_apple = (apple_r, apple_c)\n",
    "        return closest_apple\n",
    "\n",
    "    def _is_safe_move(self, next_head_r, next_head_c):\n",
    "        if not (0 <= next_head_r < self.game.height and \\\n",
    "                0 <= next_head_c < self.game.width):\n",
    "            return False\n",
    "\n",
    "        if (next_head_r, next_head_c) in self.game.snake[1:]:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def choose_action(self):\n",
    "        target_apple_pos = self._get_closest_apple_pos()\n",
    "\n",
    "        if target_apple_pos is None:\n",
    "            return 0\n",
    "\n",
    "        head_r, head_c = self.game.snake[0]\n",
    "        current_game_direction = self.game.direction\n",
    "\n",
    "        candidate_actions = []\n",
    "\n",
    "        for action_value in [-1, 0, 1]:\n",
    "            next_game_direction = (current_game_direction + action_value) % 4\n",
    "            if next_game_direction < 0: next_game_direction += 4\n",
    "\n",
    "            dr, dc = self.delta_coords[next_game_direction]\n",
    "\n",
    "            next_head_r, next_head_c = head_r + dr, head_c + dc\n",
    "\n",
    "            if self._is_safe_move(next_head_r, next_head_c):\n",
    "                dist_to_apple = abs(target_apple_pos[0] - next_head_r) + \\\n",
    "                                abs(target_apple_pos[1] - next_head_c)\n",
    "                candidate_actions.append({\n",
    "                    'action': action_value, \n",
    "                    'distance': dist_to_apple\n",
    "                })\n",
    "        \n",
    "        if not candidate_actions:\n",
    "            return 0\n",
    "\n",
    "        def sort_key(candidate):\n",
    "            action = candidate['action']\n",
    "            preference = 0\n",
    "            if action == 1: preference = 1\n",
    "            elif action == -1: preference = 2\n",
    "            return (candidate['distance'], preference)\n",
    "\n",
    "        candidate_actions.sort(key=sort_key)\n",
    "        \n",
    "        return candidate_actions[0]['action']\n",
    "\n",
    "    def play_game_and_record(self, max_steps: int, video_filename: str = \"snake_heuristic_game.gif\", fps: int = 5):\n",
    "        frames = []\n",
    "        history = {\n",
    "            'board': [],\n",
    "            'reward': [],\n",
    "            'done': -1,\n",
    "            'info': []\n",
    "        }\n",
    "        board_state, reward, done, info = self.game.reset()\n",
    "        \n",
    "        frames.append((board_state * 255).astype(np.uint8))\n",
    "\n",
    "        print(f\"Starting game with heuristic. Max steps: {max_steps}. Recording to {video_filename}\")\n",
    "\n",
    "        for step_num in range(max_steps):\n",
    "            history['board'].append(board_state)\n",
    "            history['reward'].append(reward)\n",
    "            history['info'].append(info)\n",
    "            if done:\n",
    "                history['done'] = step_num\n",
    "                print(f\"Game ended prematurely at step {step_num} before taking action. Score: {info['score']}\")\n",
    "                break\n",
    "\n",
    "            action_to_take = self.choose_action()\n",
    "            board_state, reward, done, info = self.game.step(action_to_take)\n",
    "            \n",
    "            frames.append((board_state * 255).astype(np.uint8))\n",
    "            \n",
    "            if (step_num + 1) % 100 == 0:\n",
    "                 print(f\"Step {step_num+1}/{max_steps}, Score: {info['score']}, Done: {done}\")\n",
    "\n",
    "\n",
    "            \"\"\"if done:\n",
    "                print(f\"Game over at step {step_num+1}. Final Score: {info['score']}\")\n",
    "                break\"\"\"\n",
    "        else:\n",
    "            print(f\"Game finished after {max_steps} steps (max_steps reached). Final Score: {self.game.score}\")\n",
    "\n",
    "        if frames:\n",
    "            print(f\"Saving video with {len(frames)} frames at {fps} FPS...\")\n",
    "            try:\n",
    "                imageio.mimsave(video_filename, frames, fps=fps)\n",
    "                print(f\"Video saved successfully as {video_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving video: {e}\")\n",
    "                print(\"Please ensure you have an imageio backend installed (e.g., 'pip install imageio[ffmpeg]' for MP4 or 'pip install imageio pillow' for GIF).\")\n",
    "        else:\n",
    "            print(\"No frames recorded.\")\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027ca97c-a5d1-4f42-84c7-8435cd088967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzmann_action(q_values, temperature):\n",
    "    probabilities = F.softmax(q_values / temperature, dim=1)\n",
    "    return torch.multinomial(probabilities, 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64b8b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\"\"\"\n",
    "    DQN, Replay Buffer, and Helper Functions\n",
    "\"\"\"\n",
    "class SnakeDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(SnakeDQN, self).__init__()\n",
    "        C, H, W = input_shape\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(C, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        dummy_input = torch.zeros(1, *input_shape)\n",
    "        conv_out_size = self.conv(dummy_input).view(1, -1).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "def plot_training_metrics(avg_scores, losses, q_values):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(18, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(avg_scores)\n",
    "    plt.title(\"Average Scores (per 50 episodes)\")\n",
    "    plt.xlabel(\"Every 50 Episodes\")\n",
    "    plt.ylabel(\"Average Score\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Average Loss per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(q_values)\n",
    "    plt.title(\"Average Q-value per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Q-value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        return (\n",
    "            torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity=10000, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, td_error=None):\n",
    "        # Priority = abs(td_error) or max priority if td_error is None\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        priority = abs(td_error) + 1e-5 if td_error is not None else max_priority\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "\n",
    "        self.priorities[self.pos] = priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            probs = self.priorities\n",
    "        else:\n",
    "            probs = self.priorities[:len(self.buffer)]\n",
    "\n",
    "        probs = probs ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(weights, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            indices\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        for idx, error in zip(indices, td_errors):\n",
    "            self.priorities[idx] = abs(error) + 1e-5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def train_adv_dqn(env, model, using_heuristic=False, heuristic=None, input_shape=None, n_actions=None, bufferType=\"BasicReplay\"):\n",
    "    target_model = type(model)(input_shape, n_actions).to(device)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    target_model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.001\n",
    "    epsilon_decay = 0.990\n",
    "    buffer = ReplayBuffer(capacity=10000) if bufferType == \"BasicReplay\" else PrioritizedReplayBuffer(capacity=10000)\n",
    "    #buffer = PrioritizedReplayBuffer(capacity=10000)\n",
    "    scores = []\n",
    "    avg_scores = []\n",
    "    losses = []\n",
    "    q_values = []\n",
    "\n",
    "    max_train_episodes = 5000\n",
    "    update_target_every = 100\n",
    "    batch_size = 32\n",
    "    gamma = 0.99\n",
    "    beta_frames = 100000\n",
    "    global_step = 0\n",
    "\n",
    "    is_prioritized = isinstance(buffer, PrioritizedReplayBuffer)\n",
    "    print(f\"we are using {bufferType} buffer\")\n",
    "\n",
    "    # WARMUP\n",
    "    if using_heuristic and heuristic is not None:\n",
    "        print(\"Starting warmup with heuristic for 50 episodes...\")\n",
    "        for _ in range(50):\n",
    "            state, _, done, _ = env.reset()\n",
    "            state = np.transpose(state, (2, 0, 1))\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            done = False\n",
    "            steps = 0\n",
    "            while not done and steps < MAX_GAME_STEPS:\n",
    "                action = heuristic.choose_action()\n",
    "\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state_proc = np.transpose(next_state, (2, 0, 1))\n",
    "\n",
    "                buffer.add(state.cpu().squeeze(0).numpy(), [-1, 0, 1].index(action), reward, next_state_proc, float(done))\n",
    "\n",
    "                state = torch.tensor(next_state_proc, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                steps += 1\n",
    "\n",
    "        print(f\"Warmup finished. Replay buffer size: {len(buffer)}\")\n",
    "\n",
    "    for episode in range(1, max_train_episodes + 1):\n",
    "        state, _, done, _ = env.reset()\n",
    "        state = np.transpose(state, (2, 0, 1))\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        total_loss = 0\n",
    "        total_q = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done and steps < MAX_GAME_STEPS:\n",
    "            if random.random() < epsilon:\n",
    "                if using_heuristic:\n",
    "                    action = heuristic.choose_action()\n",
    "                else:\n",
    "                    action = random.choice([-1, 0, 1])\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = model(state)\n",
    "                    action_idx = q_vals.argmax().item()\n",
    "                    action = [-1, 0, 1][action_idx]\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state_proc = np.transpose(next_state, (2, 0, 1))\n",
    "            next_state_tensor = torch.tensor(next_state_proc, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            if is_prioritized:\n",
    "                with torch.no_grad():\n",
    "                    q_val = model(state).gather(1, torch.tensor([[[-1, 0, 1].index(action)]]).to(device))\n",
    "                    next_q = target_model(next_state_tensor).max(1)[0].unsqueeze(1)\n",
    "                    td_error = reward + gamma * (1 - float(done)) * next_q - q_val\n",
    "                    td_error = td_error.item()\n",
    "\n",
    "                buffer.add(\n",
    "                    state.cpu().squeeze(0).numpy(),\n",
    "                    [-1, 0, 1].index(action),\n",
    "                    reward,\n",
    "                    next_state_proc,\n",
    "                    float(done),\n",
    "                    td_error=td_error\n",
    "                )\n",
    "            else:\n",
    "                buffer.add(\n",
    "                    state.cpu().squeeze(0).numpy(),  \n",
    "                    [-1, 0, 1].index(action),\n",
    "                    reward,\n",
    "                    next_state_proc,\n",
    "                    float(done)\n",
    "                )\n",
    "\n",
    "            state = next_state_tensor\n",
    "            steps += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # Learn from replay buffer\n",
    "            if len(buffer) >= batch_size:\n",
    "                if is_prioritized:\n",
    "                    b_states, b_actions, b_rewards, b_next_states, b_dones, weights, indices = buffer.sample(batch_size)\n",
    "                else:\n",
    "                    b_states, b_actions, b_rewards, b_next_states, b_dones = buffer.sample(batch_size)\n",
    "\n",
    "                q_values_pred = model(b_states).gather(1, b_actions)\n",
    "                with torch.no_grad():\n",
    "                    next_qs = target_model(b_next_states)\n",
    "                    max_next_qs = next_qs.max(1, keepdim=True)[0]\n",
    "                    q_target = b_rewards + gamma * (1 - b_dones) * max_next_qs\n",
    "\n",
    "                td_errors = q_target - q_values_pred\n",
    "                if is_prioritized:\n",
    "                    loss = (td_errors.pow(2) * weights).mean()\n",
    "                else:\n",
    "                    loss = nn.MSELoss()(q_values_pred, q_target)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if isinstance(buffer, PrioritizedReplayBuffer):\n",
    "                    buffer.update_priorities(indices, td_errors.detach().cpu().numpy().squeeze())\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_q += q_values_pred.mean().item()\n",
    "\n",
    "            # Target network update\n",
    "            if global_step % update_target_every == 0:\n",
    "                target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Decay epsilon after each episode\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "            \n",
    "        scores.append(info['score'])\n",
    "        losses.append(total_loss / steps if steps > 0 else 0)\n",
    "        q_values.append(total_q / steps if steps > 0 else 0)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode} | Score: {info['score']} | Loss: {losses[-1]:.4f} | Q: {q_values[-1]:.4f}\")\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            avg_score = sum(scores[-50:]) / 50\n",
    "            avg_scores.append(avg_score)\n",
    "            print(f\"Episode {episode}, Avg Score: {avg_score:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    return model, avg_scores, losses, q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a4ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are using BasicReplay buffer\n",
      "Starting warmup with heuristic for 50 episodes...\n",
      "Warmup finished. Replay buffer size: 10000\n",
      "Episode 10 | Score: 41.421999999999905 | Loss: 0.1053 | Q: 3.6590\n",
      "Episode 20 | Score: 3.8139999999999974 | Loss: 0.1984 | Q: 4.9380\n",
      "Episode 30 | Score: 6.049999999999993 | Loss: 0.2558 | Q: 6.6263\n",
      "Episode 40 | Score: 12.678000000000031 | Loss: 0.3365 | Q: 7.1496\n",
      "Episode 50 | Score: 13.712000000000032 | Loss: 0.4307 | Q: 7.8871\n",
      "Episode 50, Avg Score: 18.65, Epsilon: 0.605\n",
      "Episode 60 | Score: 4.449999999999998 | Loss: 0.3482 | Q: 8.4444\n",
      "Episode 70 | Score: 11.050000000000004 | Loss: 0.6075 | Q: 8.6952\n",
      "Episode 80 | Score: 15.906000000000029 | Loss: 0.5907 | Q: 9.4711\n",
      "Episode 90 | Score: 0.7000000000000001 | Loss: 0.8547 | Q: 9.3169\n",
      "Episode 100 | Score: 8.951999999999993 | Loss: 0.7773 | Q: 9.3227\n",
      "Episode 100, Avg Score: 7.82, Epsilon: 0.366\n",
      "Episode 110 | Score: 13.83600000000003 | Loss: 0.8746 | Q: 9.1415\n",
      "Episode 120 | Score: 1.2640000000000005 | Loss: 0.8005 | Q: 8.9794\n",
      "Episode 130 | Score: 1.3500000000000005 | Loss: 0.9148 | Q: 9.0044\n",
      "Episode 140 | Score: 0.7500000000000001 | Loss: 1.0828 | Q: 9.0061\n",
      "Episode 150 | Score: 0.7500000000000001 | Loss: 0.7751 | Q: 9.0659\n",
      "Episode 150, Avg Score: 3.59, Epsilon: 0.221\n",
      "Episode 160 | Score: 0.7500000000000001 | Loss: 1.0184 | Q: 8.5528\n",
      "Episode 170 | Score: 21.036000000000104 | Loss: 0.9504 | Q: 8.8589\n",
      "Episode 180 | Score: 5.883999999999986 | Loss: 0.9442 | Q: 8.2246\n",
      "Episode 190 | Score: 3.2959999999999967 | Loss: 0.8352 | Q: 7.8790\n",
      "Episode 200 | Score: 4.679999999999994 | Loss: 0.8454 | Q: 7.3089\n",
      "Episode 200, Avg Score: 4.35, Epsilon: 0.134\n",
      "Episode 210 | Score: 0.9000000000000002 | Loss: 0.8620 | Q: 6.6818\n",
      "Episode 220 | Score: 5.529999999999992 | Loss: 0.5642 | Q: 6.0860\n",
      "Episode 230 | Score: 9.826000000000006 | Loss: 0.3259 | Q: 5.8826\n",
      "Episode 240 | Score: 5.965999999999985 | Loss: 0.2804 | Q: 7.1501\n",
      "Episode 250 | Score: 20.752000000000145 | Loss: 0.1712 | Q: 7.5052\n",
      "Episode 250, Avg Score: 11.48, Epsilon: 0.081\n",
      "Episode 260 | Score: 24.02200000000017 | Loss: 0.1202 | Q: 7.7045\n",
      "Episode 270 | Score: 24.772000000000183 | Loss: 0.0809 | Q: 7.4735\n",
      "Episode 280 | Score: 22.340000000000185 | Loss: 0.0499 | Q: 6.7763\n",
      "Episode 290 | Score: 23.294000000000146 | Loss: 0.0276 | Q: 6.7037\n",
      "Episode 300 | Score: 22.49200000000015 | Loss: 0.0273 | Q: 6.7685\n",
      "Episode 300, Avg Score: 20.45, Epsilon: 0.049\n",
      "Episode 310 | Score: 23.724000000000164 | Loss: 0.0324 | Q: 7.0936\n",
      "Episode 320 | Score: 23.816000000000162 | Loss: 0.0364 | Q: 7.2442\n",
      "Episode 330 | Score: 23.738000000000177 | Loss: 0.0232 | Q: 7.1264\n",
      "Episode 340 | Score: 24.486000000000153 | Loss: 0.0276 | Q: 7.0764\n",
      "Episode 350 | Score: 23.83000000000018 | Loss: 0.0230 | Q: 7.2595\n",
      "Episode 350, Avg Score: 21.61, Epsilon: 0.030\n",
      "Episode 360 | Score: 25.294000000000203 | Loss: 0.0338 | Q: 7.3274\n",
      "Episode 370 | Score: 22.99400000000019 | Loss: 0.0174 | Q: 7.0980\n",
      "Episode 380 | Score: 26.682000000000237 | Loss: 0.0161 | Q: 6.8032\n",
      "Episode 390 | Score: 25.102000000000185 | Loss: 0.0283 | Q: 6.2810\n",
      "Episode 400 | Score: 26.170000000000208 | Loss: 0.0167 | Q: 6.3524\n",
      "Episode 400, Avg Score: 21.37, Epsilon: 0.018\n",
      "Episode 410 | Score: 24.082000000000164 | Loss: 0.0215 | Q: 6.2985\n",
      "Episode 420 | Score: 26.274000000000214 | Loss: 0.0151 | Q: 6.1560\n",
      "Episode 430 | Score: 23.76400000000018 | Loss: 0.0070 | Q: 5.9709\n",
      "Episode 440 | Score: 24.804000000000148 | Loss: 0.0079 | Q: 5.9859\n",
      "Episode 450 | Score: 16.016000000000076 | Loss: 0.0136 | Q: 5.9268\n",
      "Episode 450, Avg Score: 22.90, Epsilon: 0.011\n",
      "Episode 460 | Score: 24.62800000000017 | Loss: 0.0114 | Q: 6.1681\n",
      "Episode 470 | Score: 25.58200000000019 | Loss: 0.0129 | Q: 6.2447\n",
      "Episode 480 | Score: 25.5640000000002 | Loss: 0.0200 | Q: 6.2623\n",
      "Episode 490 | Score: 24.134000000000178 | Loss: 0.0039 | Q: 6.0916\n",
      "Episode 500 | Score: 23.918000000000106 | Loss: 0.0050 | Q: 6.0956\n",
      "Episode 500, Avg Score: 23.21, Epsilon: 0.007\n",
      "Episode 510 | Score: 11.244000000000023 | Loss: 0.0179 | Q: 6.0075\n",
      "Episode 520 | Score: 25.38200000000016 | Loss: 0.0229 | Q: 6.3472\n",
      "Episode 530 | Score: 24.27400000000015 | Loss: 0.0229 | Q: 6.4708\n",
      "Episode 540 | Score: 25.79600000000023 | Loss: 0.0134 | Q: 6.3330\n",
      "Episode 550 | Score: 26.462000000000213 | Loss: 0.0043 | Q: 6.4508\n",
      "Episode 550, Avg Score: 22.12, Epsilon: 0.004\n",
      "Episode 560 | Score: 25.872000000000202 | Loss: 0.0042 | Q: 6.2400\n",
      "Episode 570 | Score: 23.862000000000197 | Loss: 0.0032 | Q: 6.2030\n",
      "Episode 580 | Score: 25.4640000000002 | Loss: 0.0029 | Q: 6.0182\n",
      "Episode 590 | Score: 24.168000000000205 | Loss: 0.0026 | Q: 5.8356\n",
      "Episode 600 | Score: 24.842000000000215 | Loss: 0.0025 | Q: 5.9083\n",
      "Episode 600, Avg Score: 25.00, Epsilon: 0.002\n",
      "Episode 610 | Score: 24.018000000000097 | Loss: 0.0017 | Q: 5.8616\n",
      "Episode 620 | Score: 25.2260000000002 | Loss: 0.0042 | Q: 5.8455\n",
      "Episode 630 | Score: 25.53800000000021 | Loss: 0.0142 | Q: 5.9858\n",
      "Episode 640 | Score: 26.88400000000023 | Loss: 0.0068 | Q: 6.1309\n",
      "Episode 650 | Score: 26.624000000000205 | Loss: 0.0040 | Q: 6.1641\n",
      "Episode 650, Avg Score: 23.71, Epsilon: 0.001\n",
      "Episode 660 | Score: 12.806000000000045 | Loss: 0.0074 | Q: 6.1413\n",
      "Episode 670 | Score: 24.222000000000225 | Loss: 0.0048 | Q: 6.0289\n",
      "Episode 680 | Score: 14.070000000000052 | Loss: 0.0034 | Q: 5.8595\n",
      "Episode 690 | Score: 27.796000000000248 | Loss: 0.0045 | Q: 5.7198\n",
      "Episode 700 | Score: 26.234000000000247 | Loss: 0.0043 | Q: 5.7862\n",
      "Episode 700, Avg Score: 24.59, Epsilon: 0.001\n",
      "Episode 710 | Score: 24.874000000000215 | Loss: 0.0032 | Q: 5.7520\n",
      "Episode 720 | Score: 24.82400000000018 | Loss: 0.0036 | Q: 5.7322\n",
      "Episode 730 | Score: 24.466000000000168 | Loss: 0.0040 | Q: 5.9075\n",
      "Episode 740 | Score: 26.470000000000194 | Loss: 0.0036 | Q: 5.9301\n",
      "Episode 750 | Score: 27.698000000000224 | Loss: 0.0076 | Q: 5.8662\n",
      "Episode 750, Avg Score: 25.21, Epsilon: 0.001\n",
      "Episode 760 | Score: 25.140000000000192 | Loss: 0.0046 | Q: 5.9747\n",
      "Episode 770 | Score: 11.884000000000034 | Loss: 0.0040 | Q: 5.8836\n",
      "Episode 780 | Score: 32.47800000000019 | Loss: 0.0125 | Q: 5.8976\n",
      "Episode 790 | Score: 25.630000000000205 | Loss: 0.0058 | Q: 6.1952\n",
      "Episode 800 | Score: 28.330000000000254 | Loss: 0.0058 | Q: 6.2297\n",
      "Episode 800, Avg Score: 25.21, Epsilon: 0.001\n",
      "Episode 810 | Score: 25.818000000000183 | Loss: 0.0051 | Q: 6.2356\n",
      "Episode 820 | Score: 25.13800000000022 | Loss: 0.0046 | Q: 5.9205\n",
      "Episode 830 | Score: 28.82600000000023 | Loss: 0.0041 | Q: 5.9520\n",
      "Episode 840 | Score: 27.848000000000223 | Loss: 0.0034 | Q: 5.9955\n",
      "Episode 850 | Score: 24.904000000000185 | Loss: 0.0034 | Q: 5.9061\n",
      "Episode 850, Avg Score: 25.96, Epsilon: 0.001\n",
      "Episode 860 | Score: 28.81400000000025 | Loss: 0.0078 | Q: 5.7580\n",
      "Episode 870 | Score: 25.416000000000203 | Loss: 0.0078 | Q: 5.8467\n",
      "Episode 880 | Score: 25.818000000000197 | Loss: 0.0056 | Q: 5.7713\n",
      "Episode 890 | Score: 27.234000000000194 | Loss: 0.0042 | Q: 5.9231\n",
      "Episode 900 | Score: 27.888000000000233 | Loss: 0.0037 | Q: 5.9077\n",
      "Episode 900, Avg Score: 25.93, Epsilon: 0.001\n",
      "Episode 910 | Score: 27.06600000000025 | Loss: 0.0036 | Q: 6.0350\n",
      "Episode 920 | Score: 26.494000000000206 | Loss: 0.0082 | Q: 6.0876\n",
      "Episode 930 | Score: 27.140000000000196 | Loss: 0.0088 | Q: 6.2624\n",
      "Episode 940 | Score: 25.278000000000144 | Loss: 0.0054 | Q: 6.3643\n",
      "Episode 950 | Score: 29.000000000000238 | Loss: 0.0095 | Q: 6.3665\n",
      "Episode 950, Avg Score: 25.85, Epsilon: 0.001\n",
      "Episode 960 | Score: 28.252000000000223 | Loss: 0.0141 | Q: 6.4595\n",
      "Episode 970 | Score: 24.694000000000187 | Loss: 0.0113 | Q: 6.3913\n",
      "Episode 980 | Score: 26.094000000000204 | Loss: 0.0084 | Q: 6.4227\n",
      "Episode 990 | Score: 29.54400000000021 | Loss: 0.0123 | Q: 6.6422\n",
      "Episode 1000 | Score: 26.610000000000227 | Loss: 0.0077 | Q: 6.6348\n",
      "Episode 1000, Avg Score: 26.35, Epsilon: 0.001\n",
      "Episode 1010 | Score: 29.394000000000215 | Loss: 0.0070 | Q: 6.7520\n",
      "Episode 1020 | Score: 18.358000000000096 | Loss: 0.0216 | Q: 6.5833\n",
      "Episode 1030 | Score: 20.26600000000014 | Loss: 0.0107 | Q: 6.6379\n",
      "Episode 1040 | Score: 30.894000000000247 | Loss: 0.0144 | Q: 6.7591\n",
      "Episode 1050 | Score: 28.000000000000224 | Loss: 0.0088 | Q: 6.7241\n",
      "Episode 1050, Avg Score: 26.17, Epsilon: 0.001\n",
      "Episode 1060 | Score: 31.348000000000177 | Loss: 0.0070 | Q: 6.6807\n",
      "Episode 1070 | Score: 33.90600000000012 | Loss: 0.0064 | Q: 6.7647\n",
      "Episode 1080 | Score: 2.499999999999999 | Loss: 0.0079 | Q: 6.6771\n",
      "Episode 1090 | Score: 10.63600000000001 | Loss: 0.0100 | Q: 6.7591\n",
      "Episode 1100 | Score: 26.560000000000148 | Loss: 0.0106 | Q: 7.0863\n",
      "Episode 1100, Avg Score: 29.21, Epsilon: 0.001\n",
      "Episode 1110 | Score: 29.95200000000026 | Loss: 0.0103 | Q: 7.3103\n",
      "Episode 1120 | Score: 33.19600000000017 | Loss: 0.0090 | Q: 6.9845\n",
      "Episode 1130 | Score: 30.822000000000166 | Loss: 0.0191 | Q: 7.1470\n",
      "Episode 1140 | Score: 18.212000000000103 | Loss: 0.0120 | Q: 7.3030\n",
      "Episode 1150 | Score: 34.78400000000011 | Loss: 0.0186 | Q: 7.2887\n",
      "Episode 1150, Avg Score: 29.27, Epsilon: 0.001\n",
      "Episode 1160 | Score: 25.00000000000022 | Loss: 0.0166 | Q: 7.4692\n",
      "Episode 1170 | Score: 24.61400000000022 | Loss: 0.0129 | Q: 7.3681\n",
      "Episode 1180 | Score: 15.584000000000062 | Loss: 0.0110 | Q: 7.4386\n",
      "Episode 1190 | Score: 33.35400000000024 | Loss: 0.0175 | Q: 7.4487\n",
      "Episode 1200 | Score: 29.816000000000223 | Loss: 0.0171 | Q: 7.3506\n",
      "Episode 1200, Avg Score: 29.75, Epsilon: 0.001\n",
      "Episode 1210 | Score: 35.442000000000014 | Loss: 0.0132 | Q: 7.1017\n",
      "Episode 1220 | Score: 34.82600000000013 | Loss: 0.0128 | Q: 7.2974\n",
      "Episode 1230 | Score: 34.77600000000009 | Loss: 0.0122 | Q: 7.2441\n",
      "Episode 1240 | Score: 33.514000000000145 | Loss: 0.0152 | Q: 7.2021\n",
      "Episode 1250 | Score: 30.98800000000027 | Loss: 0.0154 | Q: 7.2104\n",
      "Episode 1250, Avg Score: 30.17, Epsilon: 0.001\n",
      "Episode 1260 | Score: 34.74200000000006 | Loss: 0.0184 | Q: 7.3433\n",
      "Episode 1270 | Score: 33.05200000000015 | Loss: 0.0235 | Q: 7.5655\n",
      "Episode 1280 | Score: 31.888000000000275 | Loss: 0.0182 | Q: 7.4871\n",
      "Episode 1290 | Score: 38.46999999999998 | Loss: 0.0133 | Q: 7.4847\n",
      "Episode 1300 | Score: 33.48200000000011 | Loss: 0.0132 | Q: 7.4366\n",
      "Episode 1300, Avg Score: 31.08, Epsilon: 0.001\n",
      "Episode 1310 | Score: 37.56399999999998 | Loss: 0.0201 | Q: 7.7196\n",
      "Episode 1320 | Score: 38.65999999999993 | Loss: 0.0156 | Q: 7.8070\n",
      "Episode 1330 | Score: 34.02000000000008 | Loss: 0.0202 | Q: 7.7140\n",
      "Episode 1340 | Score: 35.55000000000003 | Loss: 0.0309 | Q: 7.7547\n",
      "Episode 1350 | Score: 35.73999999999999 | Loss: 0.0182 | Q: 8.1375\n",
      "Episode 1350, Avg Score: 33.58, Epsilon: 0.001\n",
      "Episode 1360 | Score: 38.52599999999995 | Loss: 0.0148 | Q: 7.7677\n",
      "Episode 1370 | Score: 34.75200000000014 | Loss: 0.0199 | Q: 7.5934\n",
      "Episode 1380 | Score: 36.62000000000001 | Loss: 0.0153 | Q: 7.8982\n",
      "Episode 1390 | Score: 33.134000000000185 | Loss: 0.0167 | Q: 7.8698\n",
      "Episode 1400 | Score: 33.32400000000014 | Loss: 0.0174 | Q: 7.9091\n",
      "Episode 1400, Avg Score: 33.88, Epsilon: 0.001\n",
      "Episode 1410 | Score: 37.749999999999936 | Loss: 0.0163 | Q: 8.0266\n",
      "Episode 1420 | Score: 37.62400000000002 | Loss: 0.0179 | Q: 8.3055\n",
      "Episode 1430 | Score: 29.98200000000022 | Loss: 0.0161 | Q: 8.3930\n",
      "Episode 1440 | Score: 31.882000000000232 | Loss: 0.0154 | Q: 8.1208\n",
      "Episode 1450 | Score: 33.60000000000019 | Loss: 0.0210 | Q: 8.0808\n",
      "Episode 1450, Avg Score: 33.18, Epsilon: 0.001\n",
      "Episode 1460 | Score: 36.87199999999998 | Loss: 0.0128 | Q: 7.9832\n",
      "Episode 1470 | Score: 36.35200000000009 | Loss: 0.0182 | Q: 8.0979\n",
      "Episode 1480 | Score: 40.58599999999995 | Loss: 0.0163 | Q: 8.2661\n",
      "Episode 1490 | Score: 43.55199999999975 | Loss: 0.0175 | Q: 8.3305\n",
      "Episode 1500 | Score: 44.8999999999998 | Loss: 0.0209 | Q: 8.8014\n",
      "Episode 1500, Avg Score: 35.35, Epsilon: 0.001\n",
      "Episode 1510 | Score: 36.55999999999999 | Loss: 0.0148 | Q: 8.7782\n",
      "Episode 1520 | Score: 41.755999999999766 | Loss: 0.0151 | Q: 8.6372\n",
      "Episode 1530 | Score: 39.00399999999994 | Loss: 0.0193 | Q: 8.2686\n",
      "Episode 1540 | Score: 34.64800000000015 | Loss: 0.0217 | Q: 8.2507\n",
      "Episode 1550 | Score: 38.534000000000006 | Loss: 0.0208 | Q: 8.0716\n",
      "Episode 1550, Avg Score: 35.88, Epsilon: 0.001\n",
      "Episode 1560 | Score: 32.8220000000001 | Loss: 0.0202 | Q: 8.2737\n",
      "Episode 1570 | Score: 30.770000000000195 | Loss: 0.0184 | Q: 8.3277\n",
      "Episode 1580 | Score: 26.780000000000243 | Loss: 0.0239 | Q: 8.2734\n",
      "Episode 1590 | Score: 24.932000000000173 | Loss: 0.0193 | Q: 8.0412\n",
      "Episode 1600 | Score: 41.623999999999896 | Loss: 0.0165 | Q: 7.9266\n",
      "Episode 1600, Avg Score: 33.01, Epsilon: 0.001\n",
      "Episode 1610 | Score: 39.76400000000001 | Loss: 0.0183 | Q: 8.4071\n",
      "Episode 1620 | Score: 39.315999999999974 | Loss: 0.0163 | Q: 8.4061\n",
      "Episode 1630 | Score: 40.56799999999996 | Loss: 0.0202 | Q: 8.1555\n",
      "Episode 1640 | Score: 38.31399999999989 | Loss: 0.0348 | Q: 8.6070\n",
      "Episode 1650 | Score: 45.54399999999975 | Loss: 0.0195 | Q: 8.8098\n",
      "Episode 1650, Avg Score: 35.56, Epsilon: 0.001\n",
      "Episode 1660 | Score: 37.156000000000006 | Loss: 0.0263 | Q: 8.5975\n"
     ]
    }
   ],
   "source": [
    "game_instance = SnakeGame(\n",
    "    width=GAME_WIDTH, \n",
    "    height=GAME_HEIGHT, \n",
    "    food_amount=FOOD_AMOUNT, \n",
    "    border=BORDER_SIZE, \n",
    "    grass_growth=GRASS_GROWTH, \n",
    "    max_grass=MAX_GRASS)\n",
    "state = game_instance.reset()[0]\n",
    "state = np.transpose(state, (2, 0, 1))\n",
    "state_shape = state.shape\n",
    "model = SnakeDQN(state_shape, n_actions=3).to(device)\n",
    "\n",
    "heuristic = SnakeHeuristic(game_instance)\n",
    "\n",
    "trained_model, avg_scores, losses, q_values = train_adv_dqn(\n",
    "    game_instance, \n",
    "    model, \n",
    "    using_heuristic=True, \n",
    "    heuristic=heuristic,\n",
    "    input_shape=state_shape, \n",
    "    n_actions=3\n",
    ")\n",
    "\n",
    "plot_training_metrics(avg_scores, losses, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73052b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_instance = SnakeGame(\n",
    "    width=GAME_WIDTH, \n",
    "    height=GAME_HEIGHT, \n",
    "    food_amount=FOOD_AMOUNT, \n",
    "    border=BORDER_SIZE, \n",
    "    grass_growth=GRASS_GROWTH, \n",
    "    max_grass=MAX_GRASS)\n",
    "state = game_instance.reset()[0]\n",
    "state = np.transpose(state, (2, 0, 1))\n",
    "state_shape = state.shape\n",
    "model = SnakeDQN(state_shape, n_actions=3).to(device)\n",
    "\n",
    "heuristic = SnakeHeuristic(game_instance)\n",
    "\n",
    "trained_model, avg_scores, losses, q_values = train_adv_dqn(\n",
    "    game_instance, \n",
    "    model, \n",
    "    using_heuristic=True, \n",
    "    heuristic=heuristic,\n",
    "    input_shape=state_shape, \n",
    "    n_actions=3\n",
    "    bufferType=\"PriorityReplay\"\n",
    ")\n",
    "\n",
    "plot_training_metrics(avg_scores, losses, q_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
